The output of emotions.py run on the folder: individual_round
The Cohen kappa score is: 0.1642788920725884

The confusion matrix is the following:
[[1 0 0 2 0 0 0 0 1 0]
 [2 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 0 0 1 1 0 3]
 [3 0 0 6 0 0 0 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 1]
 [0 1 0 4 0 3 7 0 1 1]
 [1 0 0 2 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [1 0 0 0 0 0 2 0 1 0]]

This kappa score is quite low. This means our annotations are not 
always similar. This was because, as we found out later, we had quite the same thoughts about the emotions in the 
sentence, but our annotations were just not quite the same. 


Recommendations for annotations tweets: 
 
Some tweets had the possibility of being sarcastic, there was not a label for this 'emotion'.
In this case often happy words were used to describe feelings, but it was not always sure wheter
these emotions were true or not, and if they were, it was unclear how to annotate them. 
Some tweets could also been seen as sarcasm or not, but this would depend on the person writing the tweet. There were
no guidelines for such cases.
So it can be possible that these tweets were annotated wrong and did not express joy or happiness at all,
but rather disgust or anger. 

As a recommendation to solve this issue: if an annotator is able to spot the sarcasm, there could be another tag on top of
the already existing emotion to show the sarcasm in the tweet. We do not know if this is feasible, but at first thought this could 
fix the issue we encountered.